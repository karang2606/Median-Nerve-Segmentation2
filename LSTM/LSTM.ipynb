{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293bad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "from .TimeDistributed import TimeDistributed\n",
    "from .ConvLSTM import ConvLSTM\n",
    "from .unet_parts import *\n",
    "\n",
    "class LSTM_UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, num_filter = 64, bilinear=False):\n",
    "        super(LSTM_UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = TimeDistributed(DoubleConv(n_channels, num_filter))\n",
    "        self.down1 = TimeDistributed(Down(num_filter, num_filter*2))\n",
    "        self.down2 = TimeDistributed(Down(num_filter*2, num_filter*4))\n",
    "        self.down3 = TimeDistributed(Down(num_filter*4, num_filter*8))\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = TimeDistributed(Down(num_filter*8, num_filter*16 // factor))\n",
    "        \n",
    "        self.lstm = ConvLSTM(input_dim = num_filter*16 // factor,\n",
    "                    hidden_dim=num_filter*16 // factor,\n",
    "                    kernel_size=(3,3),\n",
    "                    num_layers=2,\n",
    "                    batch_first=True,\n",
    "                    bias=True,\n",
    "                    return_all_layers=False)\n",
    "        \n",
    "        self.lf1x1 = nn.Conv2d(num_filter*16, num_filter*8, kernel_size=1)\n",
    "        self.lstm1x1 = nn.Conv2d(num_filter*16, num_filter*8, kernel_size=1)\n",
    "        \n",
    "        self.up1 = (Up(num_filter*16, num_filter*8 // factor, bilinear))\n",
    "        self.up2 = (Up(num_filter*8, num_filter*4 // factor, bilinear))\n",
    "        self.up3 = (Up(num_filter*4, num_filter*2 // factor, bilinear))\n",
    "        self.up4 = (Up(num_filter*2, num_filter, bilinear))\n",
    "        self.outc = (OutConv(num_filter, n_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        _ , [(h, _)] = self.lstm(x5)\n",
    "        \n",
    "        x6 = self.lf1x1(x5[:,-1,...])\n",
    "    \n",
    "        x = self.lstm1x1(h)\n",
    "        \n",
    "        x = self.up1(torch.cat([x,x6], dim=1), x4[:,-1,...])\n",
    "        x = self.up2(x, x3[:,-1,...])\n",
    "        x = self.up3(x, x2[:,-1,...])\n",
    "        x = self.up4(x, x1[:,-1,...])\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b6fa4eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "from TimeDistributed import TimeDistributed\n",
    "from ConvLSTM import ConvLSTM\n",
    "from unet_parts import *\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_ch, out_ch, in_ch // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_ch, in_ch//2)\n",
    "        self.conv2 = nn.Conv2d(in_ch//2, out_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = self.up(x)\n",
    "        return self.conv2(x)\n",
    "    \n",
    "class LSTM_UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, num_filter=64, bilinear=False):\n",
    "        \n",
    "        n = [num_filter*2**i for i in range(4)] #[64,128,256,512,512]\n",
    "        n.append(n[-1])\n",
    "        t_filter = [2,8,32,128]\n",
    "        n_filter = [n[i] - t_filter[i] for i in range(4)] #[62,120,224,384]\n",
    "        \n",
    "        super(LSTM_UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "#         self.inc = TimeDistributed(DoubleConv(1, n_filter[0]))\n",
    "#         self.down1 = TimeDistributed(Down(n_filter[0], n_filter[1]))\n",
    "#         self.down2 = TimeDistributed(Down(n_filter[1], n_filter[2]))\n",
    "#         self.down3 = TimeDistributed(Down(n_filter[2], n_filter[3]))\n",
    "#         factor = 2 if bilinear else 1\n",
    "#         self.down4 = TimeDistributed(Down(n_filter[3], n[4]))\n",
    "        \n",
    "        self.inc = (DoubleConv(1, n_filter[0]))\n",
    "        self.down1 = (Down(n_filter[0], n_filter[1]))\n",
    "        self.down2 = (Down(n_filter[1], n_filter[2]))\n",
    "        self.down3 = (Down(n_filter[2], n_filter[3]))\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = (Down(n_filter[3], n[4]))\n",
    "        \n",
    "        self.inc_nograd = TimeDistributed(self.inc)\n",
    "        \n",
    "        \n",
    "        self.lstm1 = ConvLSTM_BLock(n_filter[0], t_filter[0])\n",
    "        self.lstm2 = ConvLSTM_BLock(n_filter[1], t_filter[1])\n",
    "        self.lstm3 = ConvLSTM_BLock(n_filter[2], t_filter[2])\n",
    "        self.lstm4 = ConvLSTM_BLock(n_filter[3], t_filter[3])\n",
    "        self.lstm5 = ConvLSTM_BLock(n[4], n[4])\n",
    "        \n",
    "        # n = [64,128,256,512,512]\n",
    "        self.up4 = (Up(n[3]*2, n[3], bilinear))\n",
    "        self.up3 = (Up(n[3]*2, n[2], bilinear))\n",
    "        self.up2 = (Up(n[2]*2, n[1], bilinear))\n",
    "        self.up1 = (Up(n[1]*2, n[0], bilinear))\n",
    "        self.outc = (OutConv(n[0]*2, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x1 = self.inc(x[:,-1,...])\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "from TimeDistributed import TimeDistributed\n",
    "from ConvLSTM import ConvLSTM\n",
    "from unet_parts import *\n",
    "\n",
    "class LSTM_UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, num_filter = 64, bilinear=False):\n",
    "        super(LSTM_UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = (DoubleConv(n_channels, num_filter))\n",
    "        self.down1 = (Down(num_filter, num_filter*2))\n",
    "        self.down2 = (Down(num_filter*2, num_filter*4))\n",
    "        self.down3 = (Down(num_filter*4, num_filter*8))\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = (Down(num_filter*8, num_filter*16 // factor))\n",
    "        \n",
    "        self.lstm = ConvLSTM(input_dim = num_filter*16 // factor,\n",
    "                    hidden_dim=num_filter*16 // factor,\n",
    "                    kernel_size=(3,3),\n",
    "                    num_layers=2,\n",
    "                    batch_first=True,\n",
    "                    bias=True,\n",
    "                    return_all_layers=False)\n",
    "        \n",
    "        self.lf1x1 = nn.Conv2d(num_filter*16, num_filter*8, kernel_size=1)\n",
    "        self.lstm1x1 = nn.Conv2d(num_filter*16, num_filter*8, kernel_size=1)\n",
    "        \n",
    "        self.up1 = (Up(num_filter*16, num_filter*8 // factor, bilinear))\n",
    "        self.up2 = (Up(num_filter*8, num_filter*4 // factor, bilinear))\n",
    "        self.up3 = (Up(num_filter*4, num_filter*2 // factor, bilinear))\n",
    "        self.up4 = (Up(num_filter*2, num_filter, bilinear))\n",
    "        self.outc = (OutConv(num_filter, n_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x[:,-1,...])\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x1_ = TimeDistributed(self.inc)(x[:,:-1,...].contiguous())\n",
    "            x2_ = TimeDistributed(self.down1)(x1_)\n",
    "            x3_ = TimeDistributed(self.down2)(x2_)\n",
    "            x4_ = TimeDistributed(self.down3)(x3_)\n",
    "            x5_ = TimeDistributed(self.down4)(x4_)\n",
    "            \n",
    "        _ , [(h, _)] = self.lstm(torch.cat((x5_,x5.unsqueeze(1)),1))\n",
    "        \n",
    "        x6 = self.lf1x1(x5)\n",
    "    \n",
    "        x = self.lstm1x1(h)\n",
    "        \n",
    "        x = self.up1(torch.cat([x,x6], dim=1), x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "        \n",
    "        t1 = self.lstm1(torch.cat((x1_,x1.unsqueeze(1)),1))\n",
    "        t2 = self.lstm2(torch.cat((x2_,x2.unsqueeze(1)),1))\n",
    "        t3 = self.lstm3(torch.cat((x3_,x3.unsqueeze(1)),1))\n",
    "        t4 = self.lstm4(torch.cat((x4_,x4.unsqueeze(1)),1))\n",
    "        t5 = self.lstm5(torch.cat((x5_,x5.unsqueeze(1)),1))\n",
    "        \n",
    "        x = self.up4([x5,t5])\n",
    "        x = self.up3([x4,t4,x])\n",
    "        x = self.up2([x3,t3,x])\n",
    "        x = self.up1([x2,t2,x])\n",
    "\n",
    "        logits = self.outc(torch.cat([x1,t1,x], dim = 1))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "121f130b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 448, 336])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from LSTM_v1 import LSTM_UNet\n",
    "model = LSTM_UNet(n_channels=1, n_classes=1, num_filter=32)\n",
    "a = torch.rand([4,10,1,448,336])\n",
    "\n",
    "out = model(a)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f38573c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.777953"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "params/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b0472221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 9, 32, 448, 336])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand([4,10,1,448,336])\n",
    "b = a[:,:-1,:,:,:].contiguous()\n",
    "l = DoubleConv(1, 32)\n",
    "t = TimeDistributed(l)(b)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c198d1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 1, 448, 336])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8ebaffc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3433942/2020758433.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m448\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m336\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "a = torch.rand([4,10,1,448,336])\n",
    "t(a[:,:-1,...]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4da279a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108.602731"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "params/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91d8bff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 448, 336])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "10b270e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 448, 336])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand([1,10,62,448,336])\n",
    "\n",
    "l = ConvLSTM_BLock(62,2)\n",
    "l(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "05a0684a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003072"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "up = ConvLSTM(input_dim = 1,\n",
    "                    hidden_dim=4,\n",
    "                    kernel_size=(3,3),\n",
    "                    num_layers=3,\n",
    "                    batch_first=True,\n",
    "                    bias=True,\n",
    "                    return_all_layers=False)\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, up.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "params/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5406fa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x], _ = up(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372aeb85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b81994f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 4, 448, 336])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc91375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57744a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(conv_block,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class up_conv(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(up_conv,self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(ch_in,ch_out,kernel_size=3,stride=1,padding=1,bias=True),\n",
    "\t\t    nn.BatchNorm2d(ch_out),\n",
    "\t\t\tnn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "    \n",
    "class Attention_block(nn.Module):\n",
    "    def __init__(self,F_g,F_l,F_int):\n",
    "        super(Attention_block,self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "            )\n",
    "        \n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self,g,x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1+x1)\n",
    "        psi = self.psi(psi)\n",
    "\n",
    "        return x*psi\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "from TimeDistributed import TimeDistributed\n",
    "from ConvLSTM import ConvLSTM\n",
    "from unet_parts import *\n",
    "\n",
    "class Att_LSTM_UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, num_filter = 32, bilinear=False):\n",
    "        super(Att_LSTM_UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = TimeDistributed(DoubleConv(n_channels, num_filter))\n",
    "        self.down1 = TimeDistributed(Down(num_filter, num_filter*2))\n",
    "        self.down2 = TimeDistributed(Down(num_filter*2, num_filter*4))\n",
    "        self.down3 = TimeDistributed(Down(num_filter*4, num_filter*8))\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = TimeDistributed(Down(num_filter*8, num_filter*16 // factor))\n",
    "        \n",
    "        self.lstm = ConvLSTM(input_dim = num_filter*16 // factor,\n",
    "                    hidden_dim=num_filter*16 // factor,\n",
    "                    kernel_size=(3,3),\n",
    "                    num_layers=2,\n",
    "                    batch_first=True,\n",
    "                    bias=True,\n",
    "                    return_all_layers=False)\n",
    "        \n",
    "        self.Up5 = up_conv(ch_in=num_filter*16,ch_out=num_filter*8)\n",
    "        self.Att5 = Attention_block(F_g=num_filter*8,F_l=num_filter*8,F_int=num_filter*4)\n",
    "        self.Up_conv5 = DoubleConv(in_channels=num_filter*16, out_channels=num_filter*8)\n",
    "\n",
    "        self.Up4 = up_conv(ch_in=num_filter*8,ch_out=num_filter*4)\n",
    "        self.Att4 = Attention_block(F_g=num_filter*4,F_l=num_filter*4,F_int=num_filter*2)\n",
    "        self.Up_conv4 = DoubleConv(in_channels=num_filter*8, out_channels=num_filter*4)\n",
    "        \n",
    "        self.Up3 = up_conv(ch_in=num_filter*4,ch_out=num_filter*2)\n",
    "        self.Att3 = Attention_block(F_g=num_filter*2,F_l=num_filter*2,F_int=num_filter)\n",
    "        self.Up_conv3 = DoubleConv(in_channels=num_filter*4, out_channels=num_filter*2)\n",
    "        \n",
    "        self.Up2 = up_conv(ch_in=num_filter*2,ch_out=num_filter)\n",
    "        self.Att2 = Attention_block(F_g=num_filter,F_l=num_filter,F_int=num_filter//2)\n",
    "        self.Up_conv2 = DoubleConv(in_channels=num_filter*2, out_channels=num_filter)\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(num_filter,n_classes,kernel_size=1,stride=1,padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        _ , [(h, c)] = self.lstm(x5)\n",
    "        print(x5.shape, h.shape)\n",
    "        # decoding + concat path\n",
    "        d5 = self.Up5(c)\n",
    "        x4 = self.Att5(g=d5,x=x4[:,-1,...])\n",
    "        d5 = torch.cat((x4,d5),dim=1)        \n",
    "        d5 = self.Up_conv5(d5)\n",
    "        \n",
    "        d4 = self.Up4(d5)\n",
    "        x3 = self.Att4(g=d4,x=x3[:,-1,...])\n",
    "        d4 = torch.cat((x3,d4),dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        x2 = self.Att3(g=d3,x=x2[:,-1,...])\n",
    "        d3 = torch.cat((x2,d3),dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        x1 = self.Att2(g=d2,x=x1[:,-1,...])\n",
    "        d2 = torch.cat((x1,d2),dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "        \n",
    "        d1 = self.Conv_1x1(d2)\n",
    "\n",
    "        return d1\n",
    "\n",
    "    def use_checkpointing(self):\n",
    "        self.inc = torch.utils.checkpoint(self.inc)\n",
    "        self.down1 = torch.utils.checkpoint(self.down1)\n",
    "        self.down2 = torch.utils.checkpoint(self.down2)\n",
    "        self.down3 = torch.utils.checkpoint(self.down3)\n",
    "        self.down4 = torch.utils.checkpoint(self.down4)\n",
    "        self.up1 = torch.utils.checkpoint(self.up1)\n",
    "        self.up2 = torch.utils.checkpoint(self.up2)\n",
    "        self.up3 = torch.utils.checkpoint(self.up3)\n",
    "        self.up4 = torch.utils.checkpoint(self.up4)\n",
    "        self.outc = torch.utils.checkpoint(self.outc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0800e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand([4,10,512,8,8])\n",
    "num_filter=32\n",
    "factor = 1\n",
    "layer = ConvLSTM(input_dim = num_filter*16 // factor,\n",
    "                    hidden_dim=num_filter*16 // factor,\n",
    "                    kernel_size=(3,3),\n",
    "                    num_layers=2,\n",
    "                    batch_first=True,\n",
    "                    bias=True,\n",
    "                    return_all_layers=False)\n",
    "[x], _ = layer(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb8875fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Att_LSTM_UNet(n_channels=1, n_classes=1, num_filter = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3d25523",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 512, 28, 21]) torch.Size([1, 512, 28, 21])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand([1,10,1,448,336])\n",
    "\n",
    "out = model(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab292d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.475389"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "params/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "17852759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 448, 336])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "690923d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 448, 336])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.squeeze(out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d43a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(conv_block,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class up_conv(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(up_conv,self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(ch_in,ch_out,kernel_size=3,stride=1,padding=1,bias=True),\n",
    "\t\t    nn.BatchNorm2d(ch_out),\n",
    "\t\t\tnn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "    \n",
    "class Attention_block(nn.Module):\n",
    "    def __init__(self,F_g,F_l,F_int):\n",
    "        super(Attention_block,self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "            )\n",
    "        \n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self,g,x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1+x1)\n",
    "        psi = self.psi(psi)\n",
    "\n",
    "        return x*psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40eb3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a =torch.rand([4,10,1,448,336])\n",
    "model = Att_LSTM_UNet(n_channels=1, n_classes=1, num_filter = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901048d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_load = './checkpoints_LSTM/checkpoint_epoch50.pth'\n",
    "\n",
    "if args_load:\n",
    "    state_dict = torch.load(args_load)#, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0598e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02d0f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.named_children():\n",
    "    if not name.startswith('params'):\n",
    "        print(name)\n",
    "        print(module)\n",
    "        print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943edcca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
