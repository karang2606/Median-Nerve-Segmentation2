{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e43fe626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.vit_seg_modeling_resnet_skip import ResNetV2\n",
    "from networks.vit_seg_modeling import CONFIGS as CONFIGS_ViT_seg\n",
    "from networks.vit_seg_modeling import VisionTransformer as ViT_seg, Embeddings\n",
    "from networks import vit_seg_configs as configs\n",
    "from torch.nn.modules.utils import _pair\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dcbea33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'R50-ViT-B_16'.find('R50') != -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b77fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea1aff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (448,336)\n",
    "config.patches.get(\"grid\")\n",
    "grid_size = config.patches[\"grid\"]\n",
    "patch_size = (img_size[0] // 16 // grid_size[0], img_size[1] // 16 // grid_size[1])\n",
    "patch_size_real = (patch_size[0] * 16, patch_size[1] * 16)\n",
    "n_patches = (img_size[0] // patch_size_real[0]) * (img_size[1] // patch_size_real[1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e73283c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 21) (1, 1) (16, 16) 588\n"
     ]
    }
   ],
   "source": [
    "print(grid_size, patch_size, patch_size_real, n_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0bd809a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4, 9) 1\n"
     ]
    }
   ],
   "source": [
    "block_units=config.resnet.num_layers\n",
    "width_factor=config.resnet.width_factor\n",
    "print(block_units, width_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0deac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand((4,3,448,336))\n",
    "emb = Embeddings(config, img_size=img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6610a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_output, features = emb(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9944e507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 588, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "edd287f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512, 56, 42])\n",
      "torch.Size([4, 256, 112, 84])\n",
      "torch.Size([4, 64, 224, 168])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(features)):\n",
    "    print(features[i].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf0ae583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a24cf55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "activation: softmax\n",
       "classifier: seg\n",
       "decoder_channels: !!python/tuple\n",
       "- 256\n",
       "- 128\n",
       "- 64\n",
       "- 16\n",
       "hidden_size: 768\n",
       "n_classes: 1\n",
       "n_skip: 3\n",
       "patch_size: 16\n",
       "patches:\n",
       "  grid: !!python/tuple\n",
       "  - 28\n",
       "  - 21\n",
       "  size: !!python/tuple\n",
       "  - 16\n",
       "  - 16\n",
       "pretrained_path: ../model/vit_checkpoint/imagenet21k/R50+ViT-B_16.npz\n",
       "representation_size: null\n",
       "resnet:\n",
       "  num_layers: !!python/tuple\n",
       "  - 3\n",
       "  - 4\n",
       "  - 9\n",
       "  width_factor: 1\n",
       "resnet_pretrained_path: null\n",
       "skip_channels:\n",
       "- 512\n",
       "- 256\n",
       "- 64\n",
       "- 16\n",
       "transformer:\n",
       "  attention_dropout_rate: 0.0\n",
       "  dropout_rate: 0.1\n",
       "  mlp_dim: 3072\n",
       "  num_heads: 12\n",
       "  num_layers: 12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = config_vit = CONFIGS_ViT_seg['R50-ViT-B_16']\n",
    "config_vit.n_classes = 1\n",
    "config_vit.n_skip = 3\n",
    "config_vit.patches.grid = (int(448 / 16), int(336 / 16))\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0df75bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ViT_seg(config_vit, img_size=(448,336), num_classes=config_vit.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f301013",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(4,1,448,336)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "762989fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 448, 336])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = net(a)\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59530957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512, 56, 42])\n",
      "torch.Size([4, 256, 112, 84])\n",
      "torch.Size([4, 64, 224, 168])\n"
     ]
    }
   ],
   "source": [
    "for f in features:\n",
    "    print(f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ee7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24bd449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden_states, attn_weights, features = net.transformer(a) # (B, n_patch, hidden)\n",
    "B, n_patch, hidden = hidden_states.size()\n",
    "h, w = config.patches.grid\n",
    "x = hidden_states.permute(0, 2, 1)\n",
    "x = x.contiguous().view(B, hidden, h, w)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7db7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "up = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "torch.cat([up(x), features[0]], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c72f5",
   "metadata": {},
   "outputs": [],
   "source": [
    ".shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d9e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "448//16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eee3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_model = ResNetV2(block_units=config.resnet.num_layers,\n",
    "                        width_factor=config.resnet.width_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c3dd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(4,3,448,336)\n",
    "out, features = hybrid_model(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de853a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c5c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (448,336)\n",
    "_pair(img_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
